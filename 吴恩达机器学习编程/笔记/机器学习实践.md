# 机器学习实践

## 1、线性回归



## 2、逻辑回归

> 是什么

- 一种分类算法，输出值在`0`到`1`之间
- 可以被视为一个很小的**神经网络**

> 梯度下降例子

- 尽量不要用显式的for循环

![image-20200818233311918](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200818233311918.png)

### 2.1 向量化

> 为什么

- 优化代码
- 提高算法效率
- 同时更新所有参数

```python
a = np.random.rand(1000000)
b = np.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print("Vecorized version:" + str(1000 * (toc - tic)) + "ms") //0.9977817535400391ms

z = 0
tic = time.time()
for i in range(1000000):
    z += a[i] * b[i]
toc = time.time()
print("For loop:" + str(1000 * (toc - tic)) + "ms") //388.85045051574707ms
```

> 向量化逻辑回归

- Z：参数和样本计算出来的值
- a ：估计值 sigmod函数
- j ：代价函数

![image-20200819000356946](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200819000356946.png)

### 2.2 Logistic Regression with a Neural Network mindset

> 学习目标

- 构造学习算法的一般架构：
  - 初始化参数
  - 计算**代价函数**及其**梯度**
  - 使用优化算法（**梯度下降**）
  - 以正确的顺序将上面的所有三个功能收集到一个**主模型功能**中

#### 1.Packages

- `numpy` ：是用Python进行科学计算的基本软件包。
- `h5py`：是与H5文件中存储的数据集进行交互的常用软件包。
- `matplotlib`：是一个著名的库，用于在Python中绘制图表。
- `lr_utils` ：在本文的资料包里，一个加载资料包里面的数据的简单功能的库。
- `PIL` and `scipy` 用来测试模型

#### 2. 数据预处理

- **Problem Statement**: 给定一个数据集 `data.h5`

  - 训练集：被标注为cat(y=1)或 non-cat(y=0)的`m_train`张图片
  - 测验集：被标注为cat或 non-cat的`m_test`张图片
  - 每张图片都是(`num_px`,`num_px`,3)
    - 3：3通道（RGB）
    - `num_px`：图片的宽度和高度（均为64x64）

- 目标是构建一个简单的图像识别算法可以正确地将图片分类为**cat**和**non-cat**

- 加载数据

  ```python
  # 加载数据 (cat/non-cat)
  # train_set_x_orig ：保存的是训练集里面的图像数据（本训练集有209张64x64的图像）
  # train_set_y_orig ：保存的是训练集的图像对应的分类值（【0 | 1】，0表示不是猫，1表示是猫）
  # test_set_x_orig ：保存的是测试集里面的图像数据（本训练集有50张64x64的图像）。
  # test_set_y_orig ： 保存的是测试集的图像对应的分类值（【0 | 1】，0表示不是猫，1表示是猫）
  # classes ： 保存的是以bytes类型保存的两个字符串数据，数据为：[b’non-cat’ b’cat’]。
  # _orig 图片需要预处理，而标签不用
  train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()
  ```

- 查看样本

  ```python
  # Example of a picture
  index = 25
  plt.imshow(train_set_x_orig[index])
  print ("y = " + str(train_set_y[:,index]) + ", it's a '" + classes[np.squeeze(train_set_y[:,index])].decode("utf-8") +  "' picture.")
  plt.show()
  ```

  <img src="G:\重要文件\备份\毕业设计\机器学习\机器学习实践\image-20200819232053098.png" alt="image-20200819232053098" style="zoom:80%;" />

- 数据参数

  ```python
  #计算样本参数
  m_train = train_set_y.size            #训练集样本数量   209
  m_test = test_set_y.size              #测试集样本数量   50
  num_px = train_set_x_orig.shape[1]    #图片尺寸        64
  #训练集的数量:       m_train = 209
  #测试集的数量:       m_test = 50
  #每张图片的宽/高:    num_px = 64
  #每张图片的大小:     (64, 64, 3)
  #训练集_图片的维数:  (209, 64, 64, 3)
#训练集_标签的维数:  (1, 209)
  #测试集_图片的维数:  (50, 64, 64, 3)
  #测试集_标签数:  (1, 50)
  ```
  
- 将[64,64,3]的图片数据**降维**成 [64 X 64 X 3]的列向量 每列代表一个图像

  ```python
  #首先将数组变为209行的矩阵,这里设为-1编译器会计算列数，此时每一行是一个图形数据， 12288
  #将矩阵转置为12288 * 209 每一列是一个图形
  train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T
  test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T
  ```

> 数据标准化

- 数据集中的每个数字代表图像中某个像素的RGB值中的一个(0-255)

- 应该标准化到[0,1]区间

  ```python
  #为每个像素指定RGB 标准化我们的数据集
  train_set_x = train_set_x_flatten / 255.
  test_set_x = test_set_x_flatten / 255.
  ```

> 数据集的预处理

- 搞清楚问题的**维度**(`m_train`,` m_test`,` num_px`, ...)
- 数据集**降维**(让每个样本是一个**列向量**)
- **标准化**数据

#### 3. 学习算法的架构

- 构建**神经网络思想**的**逻辑回归算法**

  <img src="E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200820191944971.png" alt="image-20200820191944971" style="zoom:80%;" />

- 构建步骤
  1. **初始化**模型参数
  2. 通过**最小化代价函数**学习模型参数
  3. 使用学习参数进行**预测**
  4. **分析结果**并提出结论

#### 4.  实现算法各部分

> 构建神经网络的主要步骤

1. 定义模型结构 （比如**特征数量**）
2. **初始化**模型参数
3. 循环：
   - 计算**代价函数**（正向传播）
   - 计算**梯度**（反向传播）
   - 更新**参数**（梯度下降）

##### 4.1 sigmod函数

```python
#sigmod 函数
def sigmod(z):
    '''
       参数：
          z - 任何大小的标量或numpy数组
       返回：
          sigmod(z)函数值,如果是数组，则相当于对每个元素进行sigmod计算
    '''
    return 1/ (1+np.exp(-z))
```

##### 4.2 初始化参数

```python
def initialize_with_zeros(dim):
    '''
        此函数创建一个维度为(dim,1)的0向量w，并将b初始化为0
        参数：
           dim：w向量的维度
        返回：
           w - 维度为(dim,1)的0向量
           b - 初始化标量(对应偏差)
    '''
    w = np.zeros((dim,1))
    b = 0
    # 使用断言来确保我要的数据是正确的
    assert (w.shape == (dim, 1))  #w的维度是(dim,1)
    assert (isinstance(b, float) or isinstance(b, int)) #b的类型是float或者是int
    return w, b
```

##### 4.3 前向传播和后向传播

> 目的

- 计算**代价函数**和**梯度**

```python
#使用前向传播和后向传播计算代价函数和梯度
def propagate(w,b,X,Y):
    '''
        此函数创建一个维度为(dim,1)的0向量w，并将b初始化为0
        参数：
            w -- 权重, a numpy array of size (num_px * num_px * 3, 1)
            b -- 偏差, 一个标量
            X -- data of size (num_px * num_px * 3, 训练数据数量)
            Y -- 正确的 "label" vector (containing 0 if non-cat, 1 if cat) of size (1, 训练数据数量)
            返回：
            代价 -- negative log-likelihood cost for logistic regression
            dw -- gradient of the loss with respect to w, thus same shape as w
            db -- gradient of the loss with respect to b, thus same shape as b
    '''
    m = X.shape[1] #样本数量
    # 前向传播 (从 X 计算 COST)
    z = np.dot(w.T,X)+b
    A = sigmod(z)  #m * 1

    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))

    # 反向传播 (计算梯度)
    dz = A- Y
    db = (1/m)* np.sum(dz)
    dw = (1/m)* np.dot(X,dz.T)

    # 使用断言确保数据格式是正确的
    assert (dw.shape == w.shape)
    assert (db.dtype == float)
    cost = np.squeeze(cost)   #从数组的形状中删除单维条目
    assert (cost.shape == ())
    # 创建一个字典，把dw和db保存起来
    grads = {"dw": dw,
             "db": db}

    return grads, cost
```

##### 4.4 使用梯度下降算法学习参数

- 返回了学习到的 `w`和 `b`

```python
#使用梯度下降来优化参数
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
        此函数通过运行梯度下降算法来优化w和b

        参数:
            w -- 权重, a numpy array of size (num_px * num_px * 3, 1)
            b -- 偏差, 一个标量
            X -- data of size (num_px * num_px * 3, 训练数据数量)
            Y -- 正确的 "label" vector (containing 0 if non-cat, 1 if cat) of size (1, 训练数据数量)
            num_iterations -- 优化循环迭代次数
            learning_rate -- 学习率
            print_cost -- 每循环100次输出一次代价值

        返回:
        params -- 包含权重w和偏差b的字典
        grads -- 包含权重和偏差相对于成本函数的梯度的字典
        costs -- 优化期间计算的所有成本列表，将用于绘制学习曲线

        提示：
        我们需要写下两个步骤并遍历它们：
        1）计算当前参数的成本和梯度，使用propagate（）
        2）使用w和b的梯度下降法则更新参数。
        """
    costs = []
    #迭代次数循环
    for i in range(num_iterations):
        grads , cost = propagate(w,b,X,Y)  #计算当前代价和梯度
        dw = grads["dw"]
        db = grads["db"]
        #更新参数
        w = w - learning_rate * dw
        b = b - learning_rate * db
        #没迭代100次记录一次代价值
        if(i%100 == 0):
            costs.append(cost)
        if(print_cost and i%100 == 0):
            print("Cost after iteration %i: %f" % (i, cost)) #输出代价值
    #字典存放最新的参数
    params = {"w":w,
              "b":b
                  }
    grads = { "dw":dw,
              "db":db}
    return params,grads,costs
```

##### 4.5 预测

- 预测步骤
  1. 计算预测值 `a`
  2. 如果 `a = 0 (if activation <= 0.5)  a = 1 (if activation > 0.5)`
  3. 将预测值存入向量`Y_prediction`

```python
# 预测
def predict(w,b,X):
    '''
        根据学习到的参数（w,b）预测样本是属于标签0还是标签1

        参数:
            w -- 权重, a numpy array of size (num_px * num_px * 3, 1)
            b -- 偏差, 一个标量
            X -- data of size (num_px * num_px * 3, 测试数据数量)

        返回:
        Y_prediction -- 包含X中所有图片的所有预测【0 | 1】的一个numpy数组（向量）
    '''
    m = X.shape[1]  #测试集数据数量
    Y_prediction = np.zeros((1, m)) #测试集
    w = w.reshape(X.shape[0], 1) # m*1 向量
    #计算猫出现在图片中的概率
    z = np.dot(w.T,X)+b
    A = sigmod(z)
    #遍历A中的每一个概率值
    for i in range(A.shape[1]):
        # 将概率a [0，i]转换为实际预测p [0，i]并存入Y_prediction
        Y_prediction[0,i] = 1 if(A[0,i]>0.5) else 0
    # 使用断言确定输出合法性
    assert (Y_prediction.shape == (1, m))

    return Y_prediction
```

#### 5、整合并预测

```python
#逻辑回归整合函数
def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):
    """
        实现逻辑回归的函数，梯度下降算法

        参数:
        X_train -- 训练集，维度为（num_px * num_px * 3，m_train）
        Y_train -- 训练标签集，维度为（1，m_train）
        X_test -- 测试集，维度为（num_px * num_px * 3，m_test）
        Y_test -- 测试标签集，维度为（1，m_test）
        num_iterations -- 优化迭代次数
        learning_rate -- 学习率
        print_cost -- 每迭代100次打印迭代成本

        返回:
        d -- 包含有关模型信息的字典
    """
    #初始化参数
    w,b = initialize_with_zeros(X_train.shape[0])
    #学习参数
    params,grads,costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)
    #获取参数
    w = params["w"]
    b = params["b"]
    # 预测测试/训练集的例子
    Y_prediction_test = predict(w,b,X_test)
    Y_prediction_train = predict(w,b,X_train)
    #打印训练后准确性
    print("训练集准确性:{}%".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100) )
    print("测试集准确性: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))
    d = {
        "costs": costs,                                  #代价值字典
        "Y_prediction_test": Y_prediction_test,          #测试集结果
        "Y_prediciton_train": Y_prediction_train,        #训练集结果
        "w": w,
        "b": b,
        "learning_rate": learning_rate,
        "num_iterations": num_iterations
    }
```

- 测试结果

  ```python
  Cost after iteration 0: 0.693147
  Cost after iteration 100: 0.584508
  Cost after iteration 200: 0.466949
  Cost after iteration 300: 0.376007
  Cost after iteration 400: 0.331463
  Cost after iteration 500: 0.303273
  Cost after iteration 600: 0.279880
  Cost after iteration 700: 0.260042
  Cost after iteration 800: 0.242941
  Cost after iteration 900: 0.228004
  Cost after iteration 1000: 0.214820
  Cost after iteration 1100: 0.203078
  Cost after iteration 1200: 0.192544
  Cost after iteration 1300: 0.183033
  Cost after iteration 1400: 0.174399
  Cost after iteration 1500: 0.166521
  Cost after iteration 1600: 0.159305
  Cost after iteration 1700: 0.152667
  Cost after iteration 1800: 0.146542
  Cost after iteration 1900: 0.140872
  训练集准确性:99.04306220095694%
  测试集准确性: 70.0 %
  ```

- 改进

  - 正则化
  - 改用更好的优化算法 （共轭梯度法 变尺度发）

#### 6、画图

- 

```python
#绘制图
learning_rates = [0.005,0.01, 0.001, 0.0001]
models = {}
for i in learning_rates:
    print ("learning rate is: " + str(i))
    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)
    print ('\n' + "-------------------------------------------------------" + '\n')

for i in learning_rates:
    plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))

plt.ylabel('cost')
plt.xlabel('iterations')

legend = plt.legend(loc='upper center', shadow=True)
frame = legend.get_frame()
frame.set_facecolor('0.90')
plt.show()
```



![image-20200820235810451](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200820235810451.png)

## 3、神经网络

> Selu

- <img src="E:\重要文件\重要文件\备份\毕业设计\机器学习\机器学习实践\image-20200904193226518.png" alt="image-20200904193226518" style="zoom:80%;" />

```python
def selu(x):
  with ops.name_scope('elu') as scope:
    alpha = 1.6732632423543772848170429916717
    scale = 1.0507009873554804934193349852946
    return scale*tf.where(x>0.0,x,alpha*tf.nn.elu(x))
```

### 3.1 浅层神经网络

> 目标

- 构建具有**单隐藏层**的**2类分类**神经网络
- 使用具有**非线性激活**功能激活函数，例如tanh
- 计算**交叉熵损失**（损失函数）
- 实现向前和向后传播

#### 1、 Package

- `numpy` ：是用Python进行科学计算的基本软件包。
- `sklearn`：为数据挖掘和数据分析提供的简单高效的工具
- `matplotlib`：是一个著名的库，用于在Python中绘制图表
- `testCases` ：提供了一些测试示例来评估函数的正确性
- `planar_utils ` 提供了在这个任务中使用的各种有用的功能

#### 2、数据集

> 加载和查看数据集

```python
#加载和查看数据集
X, Y = load_planar_dataset()  #加载数据集

#参数
  # x    数据点
  # y    数据点
  # c    颜色(这里用数组表示颜色)
  # s    点的大小
  # cmap 为不同的标签指定不同的颜色
#
plt.scatter(X[0, :], X[1, :], c=Y, s=40,cmap=plt.cm.Spectral) #绘制散点图
plt.show()
```



![image-20200905211749252](E:\重要文件\重要文件\备份\毕业设计\机器学习\机器学习实践\image-20200905211749252.png)

- 数据参数
  - X `(2,400)`
  - Y`(1,400)`
  - 400个样本

#### 3、使用逻辑回归解决问题

- 使用**sklearn**的内置函数

- **正确率**为 47%

  ```python
  #尝试用逻辑回归解决问题
  clf = sklearn.linear_model.LogisticRegressionCV()
  clf.fit(X.T,Y.T)
  
  #画出逻辑回归的决策边界
  plot_decision_boundary(lambda x: clf.predict(x), X, Y) #绘制决策边界
  plt.title("Logistic Regression") #图标题
  LR_predictions  = clf.predict(X.T) #预测结果
  print ("逻辑回归的准确性： %d " % float((np.dot(Y, LR_predictions) +
  		np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100) +
         "% " + "(正确标记的数据点所占的百分比)")
  plt.show()
  ```

  ![image-20200905212853327](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200905212853327.png)

#### 4、神经网络模型

- 双层神经网络

  <img src="E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200906211531234.png" alt="image-20200906211531234" style="zoom:80%;" />

- 构建神经网络的一般方法
  1. **定义神经网络结构**（输入单元的数量，隐藏单元的数量等）
  2. **初始化**模型参数
  3. 循环
     - 前向传播
     - 计算代价
     - 后向传播
     - 更新参数

##### 4.1 定义神经网络结构

```python
#定义神经网络结构
def layer_sizes(X,Y):
	"""
	    Arguments:
	    X -- 输入数据集的维度 (特征数量,样本数量)
	    Y -- 标签维度 (输出数量, 样本数量)

	    Returns:
	    n_x -- 输入层大小 即特征数量
	    n_h -- 隐藏层大小   （设为4）
	    n_y -- 输出层大小
	"""
	n_x = X.shape[0]   #输入数据集的行数（每一行代表一个特征值）
	n_h = 4            #四个隐藏单元 
	n_y = Y.shape[0]   #标签集的行数 （每一行代表一个输出值）

	return (n_x,n_h,n_y)
```

##### 4.2 初始化模型参数

- 用**随机值**初始化**权重矩阵**

- 将**偏向量**初始化为零

  ```python
  #初始化模型参数
  def initialize_parameters(n_x, n_h, n_y):
  	"""
  	  Argument:
  	  n_x -- 输入层大小
  	  n_h -- 隐藏层大小
  	  n_y -- 输出层大小
  
  	  Returns:
  	  parameters -- 包含模型参数的字典:
  	                  W1 -- 维度 (n_h, n_x)
  	                  b1 -- 维度 (n_h, 1)
  	                  W2 -- 维度 (n_y, n_h)
  	                  b2 -- 维度 (n_y, 1)
  	  """
  	np.random.seed(2)  #指定一个随机种子
  	W1 = np.random.randn(n_h, n_x)* 0.01
  	b1 = np.zeros(shape=(n_h, 1))
  	W2 = np.random.randn(n_y, n_h) * 0.01
  	b2 = np.zeros(shape=(n_y, 1))
  
  	# 使用断言确保我的数据格式是正确的
  	assert (W1.shape == (n_h, n_x))
  	assert (b1.shape == (n_h, 1))
  	assert (W2.shape == (n_y, n_h))
  	assert (b2.shape == (n_y, 1))
  
  	parameters = {
  		"W1":W1,
  		"b1": b1,
  		"W2": W2,
  		"b2": b2,
  	}
  
  	return parameters
  
  ```

#### 5、循环

##### 5.1 前向传播

> 计算激活值

- 激活函数可以选择 `sigmod`或者`tanh`

  - 一般隐藏层用`tanh` (-1 , 1)
    - 输出层用 `sigmod` (0 , 1)
  
  ```python
  #前向传播
  def forward_propagation(X, parameters):
  	"""
  	    Argument:
  	    X -- 输入数据 (n_x, m)
  	    parameters -- 模型参数 (initialization function的输出)
  
  	    Returns:
  	    A2 -- 第二层(输出层)的激活值
  	    cache -- 字典： "Z1", "A1", "Z2" and "A2"
  	"""
  	W1 = parameters["W1"]
  	b1 = parameters["b1"]
  	W2 = parameters["W2"]
  	b2 = parameters["b2"]
  	# 前向传播计算A2
  	Z1 = np.dot(W1,X) + b1
  	A1 = np.tanh(Z1)
  	Z2 = np.dot(W2,A1) + b2
  	A2 = sigmoid(Z2)
  	# 使用断言确保我的数据格式是正确的
  	assert (A2.shape == (1,X.shape[1]))
  	cache = {
  		"Z1":Z1,
  		"A1": A1,
  		"Z2": Z2,
  		"A2": A2,
  	}
  
  	return (A2,cache)
  ```

> 计算代价函数

```python
#计算代价函数
def compute_cost(A2, Y, parameters):
	"""
    Arguments:
    A2 -- 第二层(输出层)的激活值 (1, number of examples)
    Y -- "true" 标签向量 (1, number of examples)
    parameters -- 模型参数 (initialization function的输出)

    Returns:
    cost -- 交叉熵代价
    """
	m = Y.shape[1]  #Y的列数 样本数量
	W1 = parameters['W1']
	W2 = parameters['W2']

	logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
	cost = - np.sum(logprobs) / m
	cost = np.squeeze(cost)

	assert (isinstance(cost, float))  #确保cost是float
    return cost

```

##### 5.2 反向传播

![image-20200904195914519](E:\重要文件\重要文件\备份\毕业设计\机器学习\机器学习实践\image-20200904195914519.png)

```python
#反向传播计算梯度
def backward_propagation(parameters, cache, X, Y):
	"""
    Arguments:
    parameters -- 模型参数 (initialization function的输出)
    cache -- 字典： "Z1", "A1", "Z2" and "A2"
    X -- 输入数据 (n_x, m)
    Y -- "true" 标签向量 (1, number of examples)

    Returns:
    grads -- 字典：包含不同参数的梯度
    """

	m = Y.shape[1]  #样本数量
	W1 = parameters["W1"]
	W2 = parameters["W2"]

	Z1 = cache["Z1"]
	A1 = cache["A1"]
	Z2 = cache["Z2"]
	A2 = cache["A2"]

	dZ2 = A2 - Y
	dW2 = (1/m) * np.dot(dZ2,A1.T)
	db2 = (1/m) * np.sum(dZ2,axis=1,keepdims=True) #将每一行的元素相加,将矩阵压缩为一列 保持矩阵的二维特性
	dZ1 = np.multiply(np.dot(W2.T,dZ2), 1-np.power(A1,2)) #数组和矩阵对应位置相乘
	dW1 = (1/m) * np.dot(dZ1,X.T)
	db1 = (1/m) * np.sum(dZ1,axis=1,keepdims=True)

	grads = {"dW1": dW1,
			 "db1": db1,
			 "dW2": dW2,
			 "db2": db2}
	return grads
```

#### 6、更新参数

- 使用`(dW1, db1, dW2, db2)`来更新`(W1, b1, W2, b2)`

  ```python
  # 更新参数（梯度下降算法）
  def update_parameters(parameters, grads, learning_rate=1.2):
      """
      Arguments:
      parameters -- 模型参数 (initialization function的输出)
      grads -- 字典：包含不同参数的梯度
  
      Returns:
      parameters -- 更新后的模型参数
       parameters -- 包含模型参数的字典:
                        W1 -- 维度 (n_h, n_x)
                        b1 -- 维度 (n_h, 1)
                        W2 -- 维度 (n_y, n_h)
                        b2 -- 维度 (n_y, 1)
      """
      # 获取原来的参数
      W1 = parameters['W1']
      b1 = parameters['b1']
      W2 = parameters['W2']
      b2 = parameters['b2']
      # 获取参数的梯度
      dW1 = grads['dW1']
      db1 = grads['db1']
      dW2 = grads['dW2']
      db2 = grads['db2']
      # 更新参数
      W1 = W1 - dW1 * learning_rate
      b1 = b1 - db1 * learning_rate
      W2 = W2 - dW2 * learning_rate
      b2 = b2 - db2 * learning_rate
  
      parameters = {"W1": W1,
                    "b1": b1,
                    "W2": W2,
                    "b2": b2}
      return parameters
  ```

#### 7、预测

- 使用前向传播预测结果

```python
# 预测结果
def predict(parameters, X):
    """
    Arguments:
    parameters -- 你训练后的模型参数
    X -- 数据集 (特征数量,样本数量)

    Returns
    predictions -- 预测结果向量 (red: 0 / blue: 1)
    """
    # 以0.5为阈值
    A2 , cache = forward_propagation(X,parameters)
    predictions = np.round(A2)
    return predictions
    
```

#### 8、测试模型

```python
X, Y = load_planar_dataset()  # 加载数据集

# 训练模型
parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
plt.title("Decision Boundary for hidden layer size " + str(4))
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')

plt.show()
```

![image-20200909223531616](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200909223531616.png)

- 增加隐藏层可能出现过拟合现象

![image-20200909224757154](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200909224757154.png)

### 3.2 深层神经网络

> 目标

- 构建两个神经网络
  - 两层神经网络
  - 多层神经网络

![image-20200914172307280](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200914172307280.png)

- 初始化参数
- 实现前向传播
- 计算代价函数
- 实现反向传播
- 更新参数

> 正向传播

![image-20200914171945379](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200914171945379.png)

> 反向传播

![image-20200914171959991](E:\CODE\MatchineLearning\吴恩达机器学习编程\笔记\机器学习实践\image-20200914171959991.png)

#### 1  Package

- `numpy` ：是用Python进行科学计算的基本软件包。
- `h5py`：是与H5文件中存储的数据集进行交互的常用软件包。
- `matplotlib`：是一个著名的库，用于在Python中绘制图表。
- `lr_utils` ：在本文的资料包里，一个加载资料包里面的数据的简单功能的库。
- `dnn_utils`：可能需要使用的函数
- `PIL` and `scipy` 用来测试模型

#### 2  数据集

- **Problem Statement**: 给定一个数据集 `data.h5`
- 训练集：被标注为cat(y=1)或 non-cat(y=0)的`m_train`张图片
  - 测验集：被标注为cat或 non-cat的`m_test`张图片
  - 每张图片都是(`num_px`,`num_px`,3)
    - 3：3通道（RGB）
    - `num_px`：图片的宽度和高度（均为64x64）
- 数据预处理同逻辑回归

#### 3 模型结构

- 2-layer NN

- L-layer NN

- 一般方法

  1. 初始化参数/定义**超参数**
  2. 循环迭代
     - 正向传播
     - 计算代价函数
     - 反向传播
     - 更新参数
  3. 使用训练得到的参数预测结果

  ![image-20200917161425445](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200917161425445.png)

- [LINEAR-> ACTIVATION]
  
  - linear_activation_forward 计算 `Z = np.dot(W,A)+b`
  - linear_activation_forward 计算`A = Relu(Z)`或`A = sigmod(z)`

#### 4、实现算法

##### 4.1 前向传播模块

- LINEAR

- LINEAR -> ACTIVATION   

- [LINEAR -> RELU] ![$\times$](https://render.githubusercontent.com/render/math?math=%5Ctimes&mode=inline) (L-1) -> LINEAR -> SIGMOID

- 代价函数
  $$
  -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}
  $$

##### 4.2 反向传播



